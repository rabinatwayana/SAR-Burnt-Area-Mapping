{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.mdpi.com/2076-3417/12/19/10077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "from rasterio.sample import sample_gen\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,classification_report,ConfusionMatrixDisplay,\\\n",
    "                            precision_score,recall_score,f1_score,roc_auc_score, roc_curve\n",
    "\n",
    "# import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import joblib\n",
    "import os\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample points and sar data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAR GeoTIFF\n",
    "sar_image_path = \"output/sar_image/eaton_sar_indices.tif\"\n",
    "\n",
    "# Load the CSV data\n",
    "# training_data_path = 'sample_1_training_data.csv'\n",
    "# training_data_path=\"sample_1_training_data_texture.csv\"\n",
    "training_data_path=\"output/sample/training_data_eaton_indices.csv\"\n",
    "\n",
    "training_data=pd.read_csv(training_data_path)\n",
    "# feature_column_names=[\"RBD_VV\", \"RBD_VH\", \"RBR_VV\", \"RBR_VH\", \"ΔRVI\",'t1','t2','t3','t4','t5','t6','t7','t8','t9','t10','t11','t12','t13','t14','t15','t16','t17','t18','t19','t20']\n",
    "# \"dTRAD\",\"dNBR\",\n",
    "# feature_column_names=[\"vh_post\",\"vv_post\",\"vh_pre\",\"vv_pre\"]\n",
    "feature_column_names=[\"RBD_VV\", \"RBD_VH\", \"RBR_VV\",\"dod_RBR_VV\", \"RBR_VH\",\"dod_RBR_VH\",\"ΔRVI\",'Δvv_vh_ratio',\"ΔRDFI\",\"RCBI\"]\n",
    "\n",
    "\n",
    "# feature_column_names=[\"dTRAD\",\"dNBR\",\"RBD_VV\", \"RBD_VH\", \"RBR_VV\",\"dod_RBR_VV\", \"RBR_VH\",\"dod_RBR_VH\",\"ΔRVI\",'Δvv_vh_ratio',\"ΔRDFI\",\"RCBI\",'t1','t2','t3','t4','t5','t6','t7','t8','t9','t10','t11','t12','t13','t14','t15','t16','t17','t18','t19','t20'] # \"ΔRVI\",'Δvv_vh_ratio',\"ΔRDFI\" \"RBD_VV\",\n",
    "\n",
    "# feature_column_names=[\"RBD_VV\", \"RBD_VH\", \"RBR_VV\", \"dod_RBR_VV\",\"RBR_VH\",\"dod_RBR_VH\",\"ΔRVI\",'Δvv_vh_ratio',\"ΔRDFI\",\"RCBI\",'t1','t2','t3','t4','t5','t6','t7','t8','t9','t10','t11','t12','t13','t14','t15','t16','t17','t18','t19','t20'] #,\"ΔRVI\",'Δvv_vh_ratio',\"ΔRDFI\",\"dTRAD\"\n",
    "\n",
    "# feature_column_names=[\"RBD_VV\", \"RBD_VH\", \"RBR_VV\", \"RBR_VH\", \"ΔRVI\",\"Δvv_vh_ratio\",\"ΔRDFI\"] #\n",
    "# feature_column_names=['t1','t2','t3','t4','t5','t6','t7','t8','t9','t10','t11','t12','t13','t14','t15','t16','t17','t18','t19','t20']\n",
    "# feature_column_names=[\"gamm_vh_pre\",\"gamm_vv_pre\",\"gamm_vh_post\",\"gamm_vv_post\",\"RBD_VV\", \"RBD_VH\", \"RBR_VV\", \"RBR_VH\", \"ΔRVI\"]\n",
    "# feature_column_names=[\"gamm_vh_pre\",\"gamm_vv_pre\",\"gamm_vh_post\",\"gamm_vv_post\",\"RBD_VV\", \"RBD_VH\", \"RBR_VV\", \"RBR_VH\", \"ΔRVI\",'t1','t2','t3','t4','t5','t6','t7','t8','t9','t10','t11','t12','t13','t14','t15','t16','t17','t18','t19','t20']\n",
    "\n",
    "class_column_name='class'\n",
    "extended_file_name=\"_reproject\"\n",
    "\n",
    "\n",
    "output_model_dir=\"output/model/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bands: 10\n"
     ]
    }
   ],
   "source": [
    "dataset = rasterio.open(sar_image_path)\n",
    "# Get the number of bands\n",
    "num_bands = dataset.count\n",
    "print(f\"Number of bands: {num_bands}\")\n",
    "\n",
    "# To get the names of the bands (if available)\n",
    "# for i in range(1, num_bands + 1):\n",
    "#     band_name = dataset.tags(i)  # Retrieve tags for the band\n",
    "#     print(f\"Band {i} name: {band_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (SAR bands) and labels (e.g., class column)\n",
    "X = training_data[feature_column_names]  # Features (SAR band values)\n",
    "Y = training_data[class_column_name]  # Labels (add 'Class' column in your CSV if available)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.689119170984456\n",
      "2\n",
      "0.689119170984456\n",
      "3\n",
      "0.689119170984456\n",
      "4\n",
      "0.689119170984456\n",
      "5\n",
      "0.689119170984456\n",
      "6\n",
      "0.689119170984456\n",
      "7\n",
      "0.6981865284974094\n",
      "8\n",
      "0.7098445595854922\n",
      "9\n",
      "0.7150259067357513\n",
      "10\n",
      "0.7202072538860104\n",
      "11\n",
      "0.7215025906735751\n",
      "12\n",
      "0.7240932642487047\n",
      "13\n",
      "0.7253886010362695\n",
      "14\n",
      "0.7292746113989638\n",
      "15\n",
      "0.7331606217616581\n",
      "16\n",
      "0.7363989637305699\n",
      "17\n",
      "0.7338082901554405\n",
      "18\n",
      "0.7344559585492227\n",
      "19\n",
      "0.7383419689119171\n",
      "20\n",
      "0.7402849740932642\n",
      "21\n",
      "0.7422279792746114\n",
      "22\n",
      "0.7435233160621761\n",
      "23\n",
      "0.7448186528497409\n",
      "24\n",
      "0.7441709844559585\n",
      "25\n",
      "0.7454663212435233\n",
      "26\n",
      "0.7454663212435233\n",
      "27\n",
      "0.7454663212435233\n",
      "28\n",
      "0.7474093264248705\n",
      "29\n",
      "0.7474093264248705\n",
      "30\n",
      "0.7487046632124352\n",
      "31\n",
      "0.7487046632124352\n",
      "32\n",
      "0.7487046632124352\n",
      "33\n",
      "0.7512953367875648\n",
      "34\n",
      "0.7525906735751295\n",
      "35\n",
      "0.7519430051813472\n",
      "36\n",
      "0.7512953367875648\n",
      "37\n",
      "0.7525906735751295\n",
      "38\n",
      "0.7525906735751295\n",
      "39\n",
      "0.7525906735751295\n",
      "40\n",
      "0.7538860103626943\n",
      "41\n",
      "0.7512953367875648\n",
      "42\n",
      "0.7519430051813472\n",
      "43\n",
      "0.7525906735751295\n",
      "44\n",
      "0.7532383419689119\n",
      "45\n",
      "0.7532383419689119\n",
      "46\n",
      "0.7525906735751295\n",
      "47\n",
      "0.7545336787564767\n",
      "48\n",
      "0.7551813471502591\n",
      "49\n",
      "0.7545336787564767\n",
      "50\n",
      "0.7551813471502591\n",
      "51\n",
      "0.7545336787564767\n",
      "52\n",
      "0.7538860103626943\n",
      "53\n",
      "0.7538860103626943\n",
      "54\n",
      "0.7545336787564767\n",
      "55\n",
      "0.7545336787564767\n",
      "56\n",
      "0.7545336787564767\n",
      "57\n",
      "0.7571243523316062\n",
      "58\n",
      "0.7551813471502591\n",
      "59\n",
      "0.7551813471502591\n",
      "60\n",
      "0.7551813471502591\n",
      "61\n",
      "0.7564766839378239\n",
      "62\n",
      "0.7551813471502591\n",
      "63\n",
      "0.7558290155440415\n",
      "64\n",
      "0.7545336787564767\n",
      "65\n",
      "0.7545336787564767\n",
      "66\n",
      "0.7551813471502591\n",
      "67\n",
      "0.7551813471502591\n",
      "68\n",
      "0.7551813471502591\n",
      "69\n",
      "0.7551813471502591\n",
      "70\n",
      "0.7545336787564767\n",
      "71\n",
      "0.7551813471502591\n",
      "72\n",
      "0.7558290155440415\n",
      "73\n",
      "0.7545336787564767\n",
      "74\n",
      "0.7545336787564767\n",
      "75\n",
      "0.7538860103626943\n",
      "76\n",
      "0.7545336787564767\n",
      "77\n",
      "0.7545336787564767\n",
      "78\n",
      "0.7545336787564767\n",
      "79\n",
      "0.7545336787564767\n",
      "80\n",
      "0.7545336787564767\n",
      "81\n",
      "0.7545336787564767\n",
      "82\n",
      "0.7545336787564767\n",
      "83\n",
      "0.7545336787564767\n",
      "84\n",
      "0.7545336787564767\n",
      "85\n",
      "0.7545336787564767\n",
      "86\n",
      "0.7545336787564767\n",
      "87\n",
      "0.7538860103626943\n",
      "88\n",
      "0.7538860103626943\n",
      "89\n",
      "0.7532383419689119\n",
      "90\n",
      "0.7532383419689119\n",
      "91\n",
      "0.7545336787564767\n",
      "92\n",
      "0.7545336787564767\n",
      "93\n",
      "0.7532383419689119\n",
      "94\n",
      "0.7538860103626943\n",
      "95\n",
      "0.7545336787564767\n",
      "96\n",
      "0.7538860103626943\n",
      "97\n",
      "0.7538860103626943\n",
      "98\n",
      "0.7538860103626943\n",
      "99\n",
      "0.7538860103626943\n",
      "100\n",
      "0.7538860103626943\n",
      "101\n",
      "0.7538860103626943\n",
      "102\n",
      "0.7538860103626943\n",
      "103\n",
      "0.7545336787564767\n",
      "104\n",
      "0.7545336787564767\n",
      "105\n",
      "0.7538860103626943\n",
      "106\n",
      "0.7538860103626943\n",
      "107\n",
      "0.7538860103626943\n",
      "Early stopping at iteration 107\n",
      "Best validation score: 0.7571243523316062\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# # Train-test split (assuming X_train and y_train are already defined)\n",
    "# # X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize the model\n",
    "# gb_model = GradientBoostingClassifier(\n",
    "#     n_estimators=1000,        # Allow a large number of trees initially\n",
    "#     learning_rate=0.05,\n",
    "#     max_depth=3,\n",
    "#     min_samples_split=10,\n",
    "#     min_samples_leaf=5,\n",
    "#     subsample=0.8,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Initialize variables for early stopping\n",
    "# best_score = -np.inf\n",
    "# patience = 50  # Number of rounds to wait for improvement\n",
    "# no_improvement_count = 0\n",
    "\n",
    "# # Loop over the number of estimators\n",
    "# for i in range(1, gb_model.n_estimators + 1):\n",
    "#     # Fit the model up to the i-th iteration\n",
    "#     gb_model.n_estimators = i\n",
    "#     print((gb_model.n_estimators))\n",
    "#     gb_model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Calculate the validation score (e.g., accuracy)\n",
    "#     val_score = gb_model.score(X_test, y_test)\n",
    "#     print(val_score)\n",
    "#     # Check if the validation score has improved\n",
    "#     if val_score > best_score:\n",
    "#         best_score = val_score\n",
    "#         no_improvement_count = 0  # Reset the counter\n",
    "#     else:\n",
    "#         no_improvement_count += 1\n",
    "    \n",
    "#     # If no improvement for 'patience' rounds, stop training\n",
    "#     if no_improvement_count >= patience:\n",
    "#         print(f\"Early stopping at iteration {i}\")\n",
    "#         break\n",
    "\n",
    "# # Final model after early stopping\n",
    "# print(f\"Best validation score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Validation Score: 0.7137305699481865\n",
      "Iteration 2, Validation Score: 0.707901554404145\n",
      "Iteration 3, Validation Score: 0.727979274611399\n",
      "Iteration 4, Validation Score: 0.7344559585492227\n",
      "Iteration 5, Validation Score: 0.7331606217616581\n",
      "Iteration 6, Validation Score: 0.7305699481865285\n",
      "Iteration 7, Validation Score: 0.7383419689119171\n",
      "Iteration 8, Validation Score: 0.7396373056994818\n",
      "Iteration 9, Validation Score: 0.7428756476683938\n",
      "Iteration 10, Validation Score: 0.7422279792746114\n",
      "Iteration 11, Validation Score: 0.7409326424870466\n",
      "Iteration 12, Validation Score: 0.7402849740932642\n",
      "Iteration 13, Validation Score: 0.7402849740932642\n",
      "Iteration 14, Validation Score: 0.741580310880829\n",
      "Iteration 15, Validation Score: 0.7428756476683938\n",
      "Iteration 16, Validation Score: 0.7441709844559585\n",
      "Iteration 17, Validation Score: 0.7448186528497409\n",
      "Iteration 18, Validation Score: 0.7441709844559585\n",
      "Iteration 19, Validation Score: 0.7474093264248705\n",
      "Iteration 20, Validation Score: 0.7467616580310881\n",
      "Iteration 21, Validation Score: 0.7480569948186528\n",
      "Iteration 22, Validation Score: 0.7493523316062176\n",
      "Iteration 23, Validation Score: 0.7487046632124352\n",
      "Iteration 24, Validation Score: 0.7493523316062176\n",
      "Iteration 25, Validation Score: 0.7461139896373057\n",
      "Iteration 26, Validation Score: 0.7487046632124352\n",
      "Iteration 27, Validation Score: 0.7454663212435233\n",
      "Iteration 28, Validation Score: 0.7474093264248705\n",
      "Iteration 29, Validation Score: 0.7467616580310881\n",
      "Iteration 30, Validation Score: 0.7461139896373057\n",
      "Iteration 31, Validation Score: 0.7441709844559585\n",
      "Iteration 32, Validation Score: 0.7480569948186528\n",
      "Iteration 33, Validation Score: 0.7487046632124352\n",
      "Iteration 34, Validation Score: 0.7454663212435233\n",
      "Iteration 35, Validation Score: 0.7467616580310881\n",
      "Iteration 36, Validation Score: 0.7480569948186528\n",
      "Iteration 37, Validation Score: 0.7448186528497409\n",
      "Iteration 38, Validation Score: 0.7461139896373057\n",
      "Iteration 39, Validation Score: 0.7461139896373057\n",
      "Iteration 40, Validation Score: 0.7454663212435233\n",
      "Iteration 41, Validation Score: 0.7454663212435233\n",
      "Iteration 42, Validation Score: 0.7461139896373057\n",
      "Iteration 43, Validation Score: 0.7435233160621761\n",
      "Iteration 44, Validation Score: 0.7441709844559585\n",
      "Iteration 45, Validation Score: 0.7461139896373057\n",
      "Iteration 46, Validation Score: 0.7474093264248705\n",
      "Iteration 47, Validation Score: 0.7454663212435233\n",
      "Iteration 48, Validation Score: 0.7467616580310881\n",
      "Iteration 49, Validation Score: 0.7454663212435233\n",
      "Iteration 50, Validation Score: 0.7474093264248705\n",
      "Iteration 51, Validation Score: 0.7454663212435233\n",
      "Iteration 52, Validation Score: 0.7467616580310881\n",
      "Iteration 53, Validation Score: 0.7474093264248705\n",
      "Iteration 54, Validation Score: 0.7461139896373057\n",
      "Iteration 55, Validation Score: 0.7474093264248705\n",
      "Iteration 56, Validation Score: 0.7467616580310881\n",
      "Iteration 57, Validation Score: 0.7474093264248705\n",
      "Iteration 58, Validation Score: 0.7474093264248705\n",
      "Iteration 59, Validation Score: 0.7474093264248705\n",
      "Iteration 60, Validation Score: 0.7480569948186528\n",
      "Iteration 61, Validation Score: 0.7480569948186528\n",
      "Iteration 62, Validation Score: 0.7487046632124352\n",
      "Iteration 63, Validation Score: 0.7487046632124352\n",
      "Iteration 64, Validation Score: 0.7487046632124352\n",
      "Iteration 65, Validation Score: 0.7480569948186528\n",
      "Iteration 66, Validation Score: 0.7480569948186528\n",
      "Iteration 67, Validation Score: 0.7474093264248705\n",
      "Iteration 68, Validation Score: 0.7461139896373057\n",
      "Iteration 69, Validation Score: 0.7454663212435233\n",
      "Iteration 70, Validation Score: 0.7461139896373057\n",
      "Iteration 71, Validation Score: 0.7461139896373057\n",
      "Iteration 72, Validation Score: 0.7467616580310881\n",
      "Early stopping at iteration 72\n",
      "Best validation score: 0.7493523316062176\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming X_train_full and y_train_full are already defined\n",
    "# # Train-test split\n",
    "# # X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize the Random Forest model with a large number of trees\n",
    "# rf_model = RandomForestClassifier(\n",
    "#     n_estimators=1000,  # Allow a large number of trees initially\n",
    "#     max_depth=10,\n",
    "#     min_samples_split=2,\n",
    "#     min_samples_leaf=1,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Initialize variables for early stopping\n",
    "# best_score = -np.inf\n",
    "# patience = 50  # Number of rounds to wait for improvement\n",
    "# no_improvement_count = 0\n",
    "\n",
    "# # Loop over the number of estimators (trees)\n",
    "# for i in range(1, rf_model.n_estimators + 1):\n",
    "#     # Incrementally fit the Random Forest model with the i-th number of trees\n",
    "#     rf_model.set_params(n_estimators=i)\n",
    "#     rf_model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Calculate the validation score (e.g., accuracy)\n",
    "#     val_score = rf_model.score(X_test, y_test)\n",
    "#     print(f\"Iteration {i}, Validation Score: {val_score}\")\n",
    "    \n",
    "#     # Check if the validation score has improved\n",
    "#     if val_score > best_score:\n",
    "#         best_score = val_score\n",
    "#         no_improvement_count = 0  # Reset the counter\n",
    "#     else:\n",
    "#         no_improvement_count += 1\n",
    "    \n",
    "#     # If no improvement for 'patience' rounds, stop training\n",
    "#     if no_improvement_count >= patience:\n",
    "#         print(f\"Early stopping at iteration {i}\")\n",
    "#         break\n",
    "\n",
    "# # Final model after early stopping\n",
    "# print(f\"Best validation score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacking model: 0.75\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Define base learners (models to be stacked)\n",
    "# base_learners = [\n",
    "#     ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "#     ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "#     ('svm', SVC(probability=True, random_state=42))\n",
    "# ]\n",
    "\n",
    "# # Meta-model (Logistic Regression)\n",
    "# meta_model = LogisticRegression()\n",
    "\n",
    "# # Create the Stacking Classifier\n",
    "# stacking_model = StackingClassifier(estimators=base_learners, final_estimator=meta_model)\n",
    "\n",
    "# # Train the Stacking model\n",
    "# stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred_stack = stacking_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the Stacking model's performance\n",
    "# accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
    "# print(f\"Accuracy of Stacking model: {accuracy_stack:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices_tex\n",
    "# RF_Model_1=RandomForestClassifier(n_estimators= 150, min_samples_split= 8, max_features= 9, max_depth= 10)\n",
    "\n",
    "#gamma_indices\n",
    "# RF_Model_1=RandomForestClassifier(n_estimators= 100, min_samples_split= 8, max_features= 20, max_depth= 8)\n",
    "\n",
    "#gamma\n",
    "# RF_Model_0=RandomForestClassifier(n_estimators= 150, min_samples_split= 15, max_features= 20, max_depth= 12)\n",
    "# RF_Model_1=RandomForestClassifier(n_estimators= 200, min_samples_split= 8, max_features= 20, max_depth= 8)\n",
    "# {'n_estimators': 150, 'min_samples_split': 20, 'max_features': 4, 'max_depth': 15}\n",
    "#indices\n",
    "# RF_Model_0=RandomForestClassifier(n_estimators= 150, min_samples_split= 4, max_features= 20, max_depth= 10)\n",
    "# RF_Model_1=RandomForestClassifier(n_estimators= 150, min_samples_split= 8, max_features= 7, max_depth= 8)\n",
    "\n",
    "\n",
    "#eaton indices\n",
    "# RF_Model_0=RandomForestClassifier(n_estimators= 200,random_state=42)\n",
    "# max_depth= 8\n",
    "# {'n_estimators': 100, 'min_samples_split': 40, 'max_features': 3, 'max_depth': 8}\n",
    "RF_Model_0=RandomForestClassifier(n_estimators= 100, min_samples_split= 40, max_features= 3,max_depth= 8,bootstrap=True)\n",
    "# RF_Model_1=RandomForestClassifier(n_estimators= 100, min_samples_split= 4, max_features= 8, max_depth= 5)\n",
    "# RF_Model_0 = RandomForestClassifier(\n",
    "#     n_estimators=100,               # Number of trees in the forest\n",
    "#     max_depth=None,                  # Trees can grow deep to capture complex patterns\n",
    "#     min_samples_split=2,             # Minimum samples required to split an internal node\n",
    "#     min_samples_leaf=1,              # Minimum samples required to be at a leaf node\n",
    "#     max_features='sqrt',             # Use square root of the total features for each split\n",
    "#     bootstrap=True,                  # Use bootstrap sampling (default)\n",
    "#     n_jobs=-1,                       # Use all available processors\n",
    "#     random_state=42                  # Ensures reproducibility\n",
    "# )\n",
    "\n",
    "# SVM_Model = svm.SVC()\n",
    "SVM_Model= LinearSVC(random_state=42, tol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "models={\n",
    "    \"RandomForest0\": RF_Model_0,\n",
    "    \"SVM_Model\": SVM_Model\n",
    "    # \"RandomForest1\": RF_Model_1,\n",
    "    # \"RandomForest2\": RF_Model_2,\n",
    "    # \"RandomForest3\": RF_Model_3,\n",
    "    # \"RandomForest4\": RF_Model_4,\n",
    "    # \"RandomForest5\": RF_Model_5,\n",
    "\n",
    "    # \"Decision Tree\":DecisionTreeClassifier(),\n",
    "    # \"Logistic Regression\":LogisticRegression()\n",
    "    # \"XGBoost\": XGB_Model\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model_name=list(models.keys())[i]\n",
    "    print(f\"***************{model_name}*********************\")\n",
    "    model=list(models.values())[i]\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # pixels_scaled = scaler.fit_transform(pixels)  # Apply scaling to the pixels\n",
    "    # X_train=scaler.fit_transform(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "    # print(\"score\",model.score(X_test,y_test))\n",
    "\n",
    "    #Make a prediction\n",
    "    y_train_pred=model.predict(X_train)\n",
    "    # y_test_pred=model.predict(scaler.transform(X_test))\n",
    "    y_test_pred=model.predict(X_test)\n",
    "\n",
    "\n",
    "    #Training set performance\n",
    "    model_train_accuracy=accuracy_score(y_train,y_train_pred)\n",
    "    model_train_f1=f1_score(y_train,y_train_pred,average='weighted')\n",
    "    model_train_precision=precision_score(y_train,y_train_pred)\n",
    "    model_train_recall=recall_score(y_train,y_train_pred)\n",
    "    model_train_roc_auc_score=roc_auc_score(y_train,y_train_pred)\n",
    "\n",
    "    #Test set performance\n",
    "    model_test_accuracy=accuracy_score(y_test,y_test_pred)\n",
    "    model_test_f1=f1_score(y_test,y_test_pred,average='weighted')\n",
    "    model_test_precision=precision_score(y_test,y_test_pred)\n",
    "    model_test_recall=recall_score(y_test,y_test_pred)\n",
    "    model_test_roc_auc_score=roc_auc_score(y_test,y_test_pred)\n",
    "\n",
    "    # print(model)\n",
    "    print('Model performance for Training set')\n",
    "    print(f\"- Accuracy: {model_train_accuracy}\")\n",
    "    print(f\"- F1 Score: {model_train_f1}\")\n",
    "    print(f\"- Precision Score: {model_train_precision}\")\n",
    "    print(f\"- Recall Score: {model_train_recall}\")\n",
    "    print(f\"- Roc Auc Score: {model_train_roc_auc_score}\")\n",
    "\n",
    "    print(\"--------------\")\n",
    "    print('Model performance for Test set')\n",
    "    print(f\"- Accuracy: {model_test_accuracy}\")\n",
    "    print(f\"- F1 Score: {model_test_f1}\")\n",
    "    print(f\"- Precision Score: {model_test_precision}\")\n",
    "    print(f\"- Recall Score: {model_test_recall}\")\n",
    "    print(f\"- Roc Auc Score: {model_test_roc_auc_score}\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    \n",
    "    # Assuming you have a trained model called `model`\n",
    "    joblib.dump(model, f'{output_model_dir}{model_name}.pkl')\n",
    "\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "    # predict(model, sar_image_path,model_name,scaler)\n",
    "    # predict(model, sar_image_path,model_name)\n",
    "\n",
    "    # predict(model, \"output/sar_image/palisades_sar_indices.tif\",model_name)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "# rf_classifier = RandomForestClassifier(n_estimators= 100)\n",
    "\n",
    "# # Train the model\n",
    "# rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = rf_classifier.score(X_test, y_test)\n",
    "# print(f\"Random Forest Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# Model performance for Training set\n",
    "# - Accuracy: 0.9043069948186528\n",
    "# - F1 Score: 0.9032436217125988\n",
    "# - Precision Score: 0.8647854203409759\n",
    "# - Recall Score: 0.8029475982532751\n",
    "# - Roc Auc Score: 0.8750005026257168\n",
    "# --------------\n",
    "# Model performance for Test set\n",
    "# - Accuracy: 0.8853626943005182\n",
    "# - F1 Score: 0.8834883526398328\n",
    "# - Precision Score: 0.8531468531468531\n",
    "# - Recall Score: 0.7625\n",
    "# - Roc Auc Score: 0.8516447368421053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series containing feature importances from the model and feature names from the training data\n",
    "\n",
    "feature_importances = pd.Series(RF_Model_0.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "# Plot a simple bar chart\n",
    "feature_importances.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr_mat_export_path=\"output/correlation/cor_mat.csv\"\n",
    "# Compute Pearson correlation matrix\n",
    "training_data_1 = training_data.drop(columns=['id','class','geometry'])\n",
    "correlation_matrix = training_data_1.corr(method='pearson').round(2)\n",
    "print(correlation_matrix)\n",
    "correlation_matrix = correlation_matrix.applymap(lambda x: x if x >= 0.7 else 0)\n",
    "correlation_matrix.to_csv(corr_mat_export_path, index=True)\n",
    "\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".1f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction for entire image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path,model_name ):\n",
    "    dataset = rasterio.open(image_path)\n",
    "    print(dataset.count)\n",
    "    # # Load the SAR GeoTIFF\n",
    "    # image_path = \"../SAR_Data_processing/11_export/palisades_indices_subset.tif\"\n",
    "    # dataset = rasterio.open(image_path)\n",
    "\n",
    "    # Read the image bands into an array (assuming it's a multi-band raster)\n",
    "    sar_bands = np.stack([dataset.read(i+1) for i in range(dataset.count)], axis=-1)\n",
    "\n",
    "    # Reshape the SAR data to a 2D array (num_pixels, num_bands)\n",
    "    height, width, num_bands = sar_bands.shape\n",
    "    pixels = sar_bands.reshape(-1, num_bands)  # Each row is a pixel\n",
    "    print(num_bands,\"num_bands\")\n",
    "\n",
    "    # Preprocess (scale) the pixel values if needed (based on training data preprocessing)\n",
    "    # scaler = StandardScaler()\n",
    "    # pixels_scaled = scaler.fit_transform(pixels)  # Apply scaling to the pixels\n",
    "\n",
    "    # Predict for each pixel\n",
    "    predictions = model.predict(pixels)\n",
    "\n",
    "    from scipy.ndimage import uniform_filter\n",
    "    # Assuming predictions are a 2D array (for an image or spatial data)\n",
    "    predictions = uniform_filter(predictions, size=7)  # size is the window size\n",
    "\n",
    "    # predictions = model.predict(scaler.transform(pixels))\n",
    "\n",
    "\n",
    "    # Reshape predictions to match the image dimensions\n",
    "    predicted_image = predictions.reshape(height, width)\n",
    "\n",
    "    # Save the predicted classes to a new file\n",
    "    output_path = f\"output/prediction/eaton_{model_name}_{extended_file_name}.tif\"\n",
    "    meta = dataset.meta\n",
    "    meta.update(dtype=rasterio.uint8, count=1)  # Assuming class labels are integers, uint8 works for this\n",
    "\n",
    "    with rasterio.open(output_path, 'w', **meta) as dest:\n",
    "        dest.write(predicted_image.astype(rasterio.uint8), 1)  # Write to the first band\n",
    "\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "    pred_dataset = rasterio.open(output_path)\n",
    "\n",
    "    # Read the first band of the image (you can adjust for multi-band images)\n",
    "    pred_band_1 = pred_dataset.read(1)\n",
    "\n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(pred_band_1, cmap='gray')  # You can adjust the colormap if needed\n",
    "    plt.colorbar()\n",
    "    plt.title(\"SAR Burnt Area Prediction Map\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_models=os.listdir(output_model_dir)\n",
    "for model_name in ml_models:\n",
    "    output_model_path = os.path.join(output_model_dir, model_name)\n",
    "    # Load the saved model\n",
    "    loaded_model = joblib.load(output_model_path)\n",
    "\n",
    "    # Use the loaded model to make predictions\n",
    "    # predictions = loaded_model.predict(X_test)\n",
    "    predict(loaded_model, sar_image_path,model_name.replace(\".pkl\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot ROC AUC Curve\n",
    "plt.figure()\n",
    "auc_models = [{\n",
    "    'label': 'Random Forest Classifier',\n",
    "    'model': RF_Model,\n",
    "    'auc':0.73\n",
    "}]\n",
    "\n",
    "for algo in auc_models:\n",
    "    model=algo['model']\n",
    "    model.fit(X_train, y_train)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    plt.plot(fpr,tpr,label=\"Random Forest Classifier ROC (area= 0.71)\")\n",
    "\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('1-Specificity (False Positive Rate)')\n",
    "    plt.ylabel('Sensitivity (True Positive Rate)')\n",
    "    plt.title('Receiver operating characteristics')\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig(\"auc.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Open the TIFF file using rasterio\n",
    "    # output_path = \"palisades_predictions.tif\"  # Replace with your TIFF file path\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fire_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
