{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burnt area segmentation using Long-Short Term Memory (LSTM) Network\n",
    "This notebook demonstrates application of  Long-Short Term Memory (LSTM) Network models to land use classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module imports\n",
    "Below is the list of libraries and modules that are required in this notebook. The 'keras' package provides the building blocks for model configuration and training. The 'img_util' contains a set of useful functions for pre-processing of raw data to create input and output data containers, compatible to the 'keras' package. In addition, it provides a set of functions for post-processing of results and visualization of prediction maps.\n",
    "The 'sio' and 'os' module were used for working with external files. The plotting of data and generation of prediction maps were achieved using plotting functionalities of 'matplotlib'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created new conda env with python 3.8 and follow the following tutoriaLl\n",
    "# https://github.com/ChaitanyaK77/Initializing-TensorFlow-Environment-on-M3-M3-Pro-and-M3-Max-Macbook-Pros.\n",
    "\n",
    "import os\n",
    "# !pwd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "dir_path = Path(r\"/Users/rabinatwayana/Rabina/CDE II/Wildfire Project/SAR-Burnt-Area-Mapping/\")\n",
    "\n",
    "if dir_path.exists():\n",
    "    os.chdir(dir_path)\n",
    "else:\n",
    "    print(\"Directory does not exist! Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models, layers, optimizers, metrics, losses, regularizers\n",
    "import img_util as util\n",
    "from scipy import io as sio\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "# Suppress TensorFlow debug and info messages\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import config\n",
    "\n",
    "TILE_SIZE=config.TILE_SIZE\n",
    "from utils import create_fish_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: []\n",
      "No GPU detected. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "# List available physical devices and check for GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", physical_devices)\n",
    "\n",
    "if physical_devices:\n",
    "    # Set memory growth to prevent memory allocation issues\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data and class labels are available in two separate Matlab files with .mat extension. Therefore, the data were loaded into Python using the 'loadmat' function, available in the 'io' module of Scipy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes={\n",
    "#     0:\"Unburnt\",\n",
    "#     1:\"Burnt\",\n",
    "#     2:\"Unlabelled\",\n",
    "#  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_image_path=\"MachineLearning/input/ground_truth/02_gt/gt_palisade.tif\"\n",
    "sar_image_path=\"LSTM/input/S1A_asc_TC_32611_clip.tif\"\n",
    "\n",
    "output_model_path=\"output/model/palisades_lstm.h5\"\n",
    "output_graph_path=\"output/graph/palisades_lstm.png\"\n",
    "# data_folder = 'Datasets'\n",
    "# data_file= 'Indian_pines_corrected'\n",
    "# gt_file = 'Indian_pines_gt'\n",
    "# data_set = sio.loadmat(os.path.join(data_folder, data_file)).get('indian_pines_corrected')\n",
    "# gt = sio.loadmat(os.path.join(data_folder, gt_file)).get('indian_pines_gt')\n",
    "\n",
    "# Checking the shape of data_set (containing image data) and gt (containing ground truth data) Numpy arrays.\n",
    "# print(data_set.shape ,gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m     gt = dataset.read() \n\u001b[32m      3\u001b[39m     gt=gt.squeeze()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrasterio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msar_image_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mrasterio/_base.pyx:450\u001b[39m, in \u001b[36mrasterio._base.DatasetBase.__exit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mrasterio/_base.pyx:441\u001b[39m, in \u001b[36mrasterio._base.DatasetBase.close\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Rabina/anaconda3/envs/fire_env/lib/python3.11/contextlib.py:607\u001b[39m, in \u001b[36mExitStack.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    604\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    605\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m received_exc \u001b[38;5;129;01mand\u001b[39;00m suppressed_exc\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    608\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Immediately unwind the context stack.\"\"\"\u001b[39;00m\n\u001b[32m    609\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "with rasterio.open(gt_image_path) as dataset:\n",
    "    gt = dataset.read() \n",
    "    gt=gt.squeeze()\n",
    "\n",
    "with rasterio.open(sar_image_path) as dataset:\n",
    "    data_set = dataset.read() \n",
    "    data_set = np.transpose(data_set, (1, 2, 0))\n",
    "\n",
    "print(data_set.shape ,gt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export gt into tif format\n",
    "\n",
    "# height, width = gt.shape\n",
    "# transform = from_origin(0, height, 1, 1)  # Change this as per your spatial reference\n",
    "\n",
    "# # Save X_loaded as a .tif file\n",
    "# with rasterio.open('temp/X_loaded.tif', 'w', driver='GTiff', count=1, dtype=gt.dtype,\n",
    "#                    height=height, width=width, crs='EPSG:4326', transform=transform) as dst:\n",
    "#     dst.write(gt, 1)\n",
    "\n",
    "# #export dataset into tiff\n",
    "# height, width, channel = data_set.shape\n",
    "# transform = from_origin(0, height, 1, 1)  # Modify as per your coordinate system\n",
    "\n",
    "# # Save data_set as a multi-band GeoTIFF file\n",
    "# with rasterio.open(\n",
    "#     'temp/dataset_loaded.tif', 'w', driver='GTiff', \n",
    "#     height=height, width=width, count=channel, \n",
    "#     dtype=data_set.dtype, crs='EPSG:4326', \n",
    "#     transform=transform\n",
    "# ) as dst:\n",
    "#     # Rasterio expects (bands, height, width), so transpose accordingly\n",
    "#     for i in range(channel):\n",
    "#         dst.write(data_set[:, :, i], i + 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a copy of the original ground truth to update\n",
    "# updated_gt = np.zeros_like(gt)\n",
    "\n",
    "# # Iterate over each class (excluding the background class)\n",
    "# for cls in np.unique(gt):\n",
    "#     if cls == 0:  # Skip the background class if you don't want to modify its samples\n",
    "#         continue\n",
    "\n",
    "#     # Get the indices (rows and columns) where the class is present\n",
    "#     class_indices = np.where(gt == cls)\n",
    "\n",
    "#     # If the number of samples is greater than 200, randomly sample 200\n",
    "#     if len(class_indices[0]) > 200:\n",
    "#         # Randomly select 200 indices from the class samples\n",
    "#         random_indices = np.random.choice(len(class_indices[0]), size=200, replace=False)\n",
    "        \n",
    "#         # Update the ground truth array with the selected samples\n",
    "#         updated_gt[class_indices[0][random_indices], class_indices[1][random_indices]] = cls\n",
    "#     else:\n",
    "#         # If there are less than or equal to 200 samples, retain all the samples\n",
    "#         # updated_gt[class_indices[0], class_indices[1]] = cls\n",
    "\n",
    "#         # Randomly select 200 indices from the class samples\n",
    "#         random_indices = np.random.choice(len(class_indices[0]), size=int(len(class_indices[0])/2), replace=False)\n",
    "        \n",
    "#         # Update the ground truth array with the selected samples\n",
    "#         updated_gt[class_indices[0][random_indices], class_indices[1][random_indices]] = cls\n",
    "\n",
    "\n",
    "# # # Print the number of samples per class in the reduced dataset\n",
    "# # for cls in np.unique(gt):\n",
    "# #     if cls == 0:  # Skip the background class\n",
    "# #         continue\n",
    "# #     class_sample_count = np.sum(gt[reduced_train_rows, reduced_train_cols] == cls)\n",
    "# #     print(f\"Class {cls}: {class_sample_count} samples\")\n",
    "\n",
    "# # Get unique class labels and their counts in ground truth (gt)\n",
    "# gt=updated_gt\n",
    "# classes, class_counts = np.unique(gt, return_counts=True)\n",
    "\n",
    "# # Display the class and the count of samples in each class\n",
    "# for cls, count in zip(classes, class_counts):\n",
    "#     print(f\"Class {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test data\n",
    "The 'data_split' function was used for splitting the data into training and test sets using 0.75 as the split ratio (75% of labeled pixels were used for training). This function ensures that all classes are represented in the training dataset (see function documentation for available split methods). In addition, it allows users to focus their analysis on certain classes and remove those pixels that are not labeled. For example, the unlabeled data are represented by 0 in the gourd truth data file. Therefore, 0 was included in 'rem_classes' list, indicating its removal from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fraction = 0.80\n",
    "# rem_classes = [2]\n",
    "# #rem_classes is the ignored class\n",
    "# (train_rows, train_cols), (test_rows, test_cols) = util.data_split(gt,\n",
    "#                                                                    train_fraction=train_fraction,\n",
    "#                                                                    rem_classes=rem_classes)\n",
    "\n",
    "\n",
    "# print('Number of training samples = {}.\\nNumber of test samples = {}.'.format(len(train_rows), len(test_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_rows)\n",
    "# print(len(train_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A portion of training data can optionally be set aside for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_fraction = 0.05\n",
    "# (train_rows_sub, train_cols_sub), (val_rows, val_cols) = util.val_split(\n",
    "#         train_rows, train_cols, gt, val_fraction=val_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and Testing Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a copy of the ground truth for visualizing purposes\n",
    "# gt_vis = np.copy(gt)\n",
    "\n",
    "# # Assign the training data a specific value (e.g., 1)\n",
    "# gt_vis[train_rows, train_cols] = 1\n",
    "\n",
    "# # Assign the testing data a different value (e.g., 2)\n",
    "# gt_vis[test_rows, test_cols] = 2\n",
    "\n",
    "# # Plot the ground truth with different colors for training and testing\n",
    "# plt.figure(figsize=(10, 10))\n",
    "\n",
    "# # Plot the training data in one color (e.g., blue)\n",
    "# plt.imshow(gt_vis, cmap='coolwarm', alpha=0.6)\n",
    "\n",
    "# # Overlay training data points as circles (with label for training)\n",
    "# plt.scatter(train_cols, train_rows, color='blue', marker='o', label=\"Training Data\", s=10)\n",
    "\n",
    "# # Overlay testing data points as squares (with label for testing)\n",
    "# plt.scatter(test_cols, test_rows, color='red', marker='o', label=\"Testing Data\", s=10)\n",
    "\n",
    "# # Add title and legend\n",
    "# plt.title(\"Training and Testing Data Visualization\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.axis(\"off\")  # Hide the axes for better visualization\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction: Case 1\n",
    "The spectral dimension of an image dataset can be reduced using Principle Component Analysis (PCA). It helps in reducing the spectral dimension significantly without losing important information. The 'reduce_dim' function takes the numpy array containing image data as its first argument and the number of reduced dimensions (i.e., an integer) or the minimum variance captured by PCA dimensions (i.e., a float) as the second argument.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_1 = util.reduce_dim(img_data=data_set, n_components=0.98)\n",
    "# # Normalize data to ensure consistent feature scaling\n",
    "# data_set_1 = util.rescale_data(data_set_1)\n",
    "# data_set_1.shape\n",
    "\n",
    "data_set_1=data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction: Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_2 = util.reduce_dim(img_data=data_set, n_components=0.99)\n",
    "# # Normalize data to ensure consistent feature scaling\n",
    "# data_set_2 = util.rescale_data(data_set_2)\n",
    "# data_set_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction: Case 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_3 = util.reduce_dim(img_data=data_set, n_components=0.999)\n",
    "# # Normalize data to ensure consistent feature scaling\n",
    "# data_set_3 = util.rescale_data(data_set_3)\n",
    "# data_set_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a value of 0.999 for the percentage of captured variance, the spectral dimension was reduced from 200 to 69 bands. The new dimensions were sorted according to their contribution to the dataset variance. The top 10 dimensions of transformed data are illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating input and target tensors\n",
    "The input and target tensors should be compatible with the type of neural network model that is used for classification. The 'create_patch' function can create inputs, compatible to both pixel inputs for [MLP](deep_learning_MLP.ipynb) models as well as patch inputs for [2-D CNN](deep_learning_2D_CNN.ipynb) and [3-D CNN](deep_learning_3D_CNN.ipynb) models. \n",
    "In this notebook, a 2-D CNN model with a 'path_size' of 5 is used for classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [135,88,172,361,79,300,211,315,257,202,176,209,155,96,121,72,48,137,304,358,374,381,103,105,159,162,214,216,246,271,273,302,268,266,292,290,319,324,231,329,327,263]\n",
    "test_ids = [146,129,123,344,378,248,132,189,242,297,321,235,356]\n",
    "val_ids = [180,233,295,192,63,376]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_indices_from_tile_ids(fishnet_tile_map, selected_tile_ids):\n",
    "    rows, cols = np.where(np.isin(fishnet_tile_map, selected_tile_ids))\n",
    "    return rows.tolist(), cols.tolist()\n",
    "\n",
    "tiles=create_fish_net(sar_image_path, gt_image_path, tile_size=TILE_SIZE,plot_fig=False)\n",
    "\n",
    "train_rows, train_cols = get_pixel_indices_from_tile_ids(tiles, train_ids)\n",
    "test_rows, test_cols = get_pixel_indices_from_tile_ids(tiles, test_ids)\n",
    "val_rows, val_cols = get_pixel_indices_from_tile_ids(tiles, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size=5 #5\n",
    "train_pixel_indices_sub = (train_rows, train_cols)\n",
    "val_pixel_indices = (val_rows, val_cols)\n",
    "test_pixel_indices = (test_rows, test_cols) \n",
    "catg_labels = np.unique([int(gt[idx[0],idx[1]]) for idx in zip(train_rows, train_cols)])\n",
    "int_to_vector_dict = util.label_2_one_hot(catg_labels) \n",
    "\n",
    "def create_train_patch(data_set):\n",
    "    train_input_sub, y_train_sub = util.create_patch(\n",
    "        data_set=data_set,\n",
    "        gt=gt,\n",
    "        pixel_indices=train_pixel_indices_sub,\n",
    "        patch_size=patch_size,\n",
    "        label_vect_dict=int_to_vector_dict)\n",
    "    return train_input_sub, y_train_sub\n",
    "    \n",
    "\n",
    "def create_val_patch(data_set):\n",
    "    val_input, y_val = util.create_patch(\n",
    "        data_set=data_set,\n",
    "        gt=gt,\n",
    "        pixel_indices=val_pixel_indices,\n",
    "        patch_size=patch_size,\n",
    "        label_vect_dict=int_to_vector_dict)\n",
    "    return val_input, y_val\n",
    "\n",
    "\n",
    "def create_test_patch(data_set):\n",
    "    test_input, y_test = util.create_patch(\n",
    "        data_set=data_set,\n",
    "        gt=gt,\n",
    "        pixel_indices=test_pixel_indices,\n",
    "        patch_size=patch_size,\n",
    "        label_vect_dict=int_to_vector_dict)\n",
    "    return test_input, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case_1\n",
    "train_input_sub_1, y_train_sub_1 = create_train_patch(data_set_1)\n",
    "val_input_1, y_val_1 = create_val_patch(data_set_1)\n",
    "test_input_1, y_test_1 = create_test_patch(data_set_1)\n",
    "\n",
    "# #case_2\n",
    "# train_input_sub_2, y_train_sub_2 = create_train_patch(data_set_2)\n",
    "# val_input_2, y_val_2 = create_val_patch(data_set_2)\n",
    "# test_input_2, y_test_2 = create_test_patch(data_set_2)\n",
    "\n",
    "# #case_3\n",
    "# train_input_sub_3, y_train_sub_3 = create_train_patch(data_set_3)\n",
    "# val_input_3, y_val_3 = create_val_patch(data_set_3)\n",
    "# test_input_3, y_test_3 = create_test_patch(data_set_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_sub_1.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LSTM model\n",
    "The network architecture consists of a 2-D convolutional layer as input layer. The input layer is followed by a maximum pooling layer with a pooling and stride size of 2. The third layer is a another 2-D convolutional layer, followed by a maximum pooling layer. The fifth layer flattens the outputs of the forth layer and pass them to a 'drop out' layer with a drop rate of 0.35. The 'drop out' layer is followed by three 'dense' layers with the last one as the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2_parm = 1e-3\n",
    "# units_1 = 2**10\n",
    "# units_2 =2**9\n",
    "# units_3=2**8\n",
    "# units_4=2**7\n",
    "# drop_rate = 0.2#0.35\n",
    "# num_catg = len(catg_labels)  # Number of categories\n",
    "\n",
    "# def create_lstm_model(input_shape):\n",
    "#     # For LSTM, we need to treat the spectral bands as time steps\n",
    "#     time_steps = input_shape[2]  # Number of spectral bands\n",
    "#     features = input_shape[0] * input_shape[1]  # Flatten height and width into features for LSTM\n",
    "#     # Building an LSTM model\n",
    "#     lstm_model = models.Sequential()\n",
    "\n",
    "#     # Reshape the input data for LSTM\n",
    "#     lstm_model.add(layers.Reshape((time_steps, features), input_shape=input_shape))\n",
    "#     lstm_model.add(layers.LSTM(units=units_4, return_sequences=True,kernel_regularizer=regularizers.l2(l2_parm)))\n",
    "\n",
    "#     # Add more LSTM layers or Dense layers if required\n",
    "#     # lstm_model.add(layers.LSTM(units=units_4, return_sequences=True, kernel_regularizer=regularizers.l2(l2_parm)))\n",
    "\n",
    "#     # Add more LSTM layers or Dense layers if required\n",
    "#     lstm_model.add(layers.LSTM(units=units_4, return_sequences=True))\n",
    "\n",
    "#     # Add more LSTM layers or Dense layers if required\n",
    "#     lstm_model.add(layers.LSTM(units=units_4, return_sequences=False))\n",
    "\n",
    "#     lstm_model.add(layer=layers.Flatten())\n",
    "#     lstm_model.add(layers.Dropout(drop_rate))\n",
    "\n",
    "#     # dense_1\n",
    "#     lstm_model.add(layer=layers.Dense(units=units_2, activation='relu'))\n",
    "#     lstm_model.add(layers.BatchNormalization())\n",
    "\n",
    "#     lstm_model.add(layer=layers.Dense(units=num_catg, activation='softmax'))\n",
    "#     # print(train_input_sub_1.shape)\n",
    "#     lstm_model.summary()\n",
    "#     return lstm_model\n",
    "\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "\n",
    "l2_parm = 1e-3\n",
    "units_1 = 2**10\n",
    "units_2 = 2**9\n",
    "units_3 = 2**8\n",
    "units_4 = 2**7\n",
    "drop_rate = 0.2\n",
    "num_catg = len(catg_labels)  # Number of classes\n",
    "\n",
    "def create_lstm_model(input_shape):  # input_shape = (5, 5, 8)\n",
    "    time_steps = 6\n",
    "    bands_per_step = 2\n",
    "    spatial_dims = input_shape[0] * input_shape[1]  # 5*5 = 25\n",
    "    features = bands_per_step * spatial_dims  # 2 * 25 = 50\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Reshape from (5, 5, 8) -> (4, 50)\n",
    "    model.add(layers.Reshape((time_steps, features), input_shape=input_shape))\n",
    "\n",
    "    model.add(layers.LSTM(units=units_4, return_sequences=True, kernel_regularizer=regularizers.l2(l2_parm)))\n",
    "    model.add(layers.LSTM(units=units_4, return_sequences=True))\n",
    "    model.add(layers.LSTM(units=units_4, return_sequences=False))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(drop_rate))\n",
    "    model.add(layers.Dense(units=units_2, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(units=num_catg, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# l2_parm = 1e-3\n",
    "# units_1 = 2**10\n",
    "# units_2 = 2**9\n",
    "# units_3 = 2**8\n",
    "# units_4 = 2**7\n",
    "# drop_rate = 0.2\n",
    "# num_catg = len(catg_labels)  # Number of categories\n",
    "\n",
    "# def create_lstm_model(input_shape):\n",
    "#     time_steps = input_shape[2]  # Number of spectral bands (this will be the time steps for LSTM)\n",
    "#     features = input_shape[0] * input_shape[1]  # Flatten height and width into features for LSTM\n",
    "\n",
    "#     # Building a CNN-LSTM model\n",
    "#     cnn_lstm_model = models.Sequential()\n",
    "\n",
    "#     # CNN Layers for feature extraction\n",
    "#     cnn_lstm_model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "#     cnn_lstm_model.add(layers.MaxPooling2D((2, 2)))  # Pooling layer with stride 2\n",
    "\n",
    "#     cnn_lstm_model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "#     cnn_lstm_model.add(layers.MaxPooling2D((2, 2)))  # Pooling layer with stride 2\n",
    "\n",
    "#     # Avoid reducing spatial dimensions too much, reduce pooling\n",
    "#     cnn_lstm_model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    \n",
    "#     # Use Global Average Pooling to reduce dimensions without losing important features\n",
    "#     cnn_lstm_model.add(layers.GlobalAveragePooling2D())  # This will result in a 1D vector of size (batch_size, 128)\n",
    "\n",
    "#     # LSTM Layers (No Reshape needed here, just pass the 1D vector to LSTM)\n",
    "#     cnn_lstm_model.add(layers.Reshape((1, 128)))  # Adjust this to have time_steps=1, features=128\n",
    "\n",
    "#     cnn_lstm_model.add(layers.LSTM(units=units_4, return_sequences=True, kernel_regularizer=regularizers.l2(l2_parm)))\n",
    "#     cnn_lstm_model.add(layers.LSTM(units=units_4, return_sequences=True))\n",
    "#     cnn_lstm_model.add(layers.LSTM(units=units_4, return_sequences=False))\n",
    "\n",
    "#     # Dropout to prevent overfitting\n",
    "#     cnn_lstm_model.add(layers.Dropout(drop_rate))\n",
    "\n",
    "#     # Dense Layers for classification\n",
    "#     cnn_lstm_model.add(layers.Dense(units=units_2, activation='relu'))\n",
    "#     cnn_lstm_model.add(layers.BatchNormalization())\n",
    "\n",
    "#     cnn_lstm_model.add(layers.Dense(units=num_catg, activation='softmax'))  # Final classification layer\n",
    "#     cnn_lstm_model.summary()\n",
    "\n",
    "#     return cnn_lstm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model and plotting training history\n",
    "The model was compiled and trained using the training, validation and test [data.](#Creating-input-and-target-tensors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Can use 'val_accuracy' if preferred\n",
    "    patience=5,            # Stop training after 5 epochs of no improvement\n",
    "    restore_best_weights=True,  # Restore the best weights from the epoch with the lowest validation loss\n",
    "    verbose=1              # Print messages when early stopping is triggered\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 2**6\n",
    "input_shape_1 = train_input_sub_1.shape[1:]  # Shape of the input data (height, width, channels)\n",
    "# input_shape_2 = train_input_sub_2.shape[1:]  # Shape of the input data (height, width, channels)\n",
    "# input_shape_3 = train_input_sub_3.shape[1:]  # Shape of the input data (height, width, channels)\n",
    "\n",
    "lstm_model_1=create_lstm_model(input_shape_1)\n",
    "# Compiling the model\n",
    "lstm_model_1.compile(optimizer=optimizers.RMSprop(learning_rate=lr),\n",
    "                 loss=losses.categorical_crossentropy,\n",
    "                 metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "# lstm_model_2=create_lstm_model(input_shape_2)\n",
    "# # Compiling the model\n",
    "# lstm_model_2.compile(optimizer=optimizers.RMSprop(learning_rate=lr),\n",
    "#                  loss=losses.categorical_crossentropy,\n",
    "#                  metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "# lstm_model_3=create_lstm_model(input_shape_3)\n",
    "# # Compiling the model\n",
    "# lstm_model_3.compile(optimizer=optimizers.RMSprop(learning_rate=lr),\n",
    "#                  loss=losses.categorical_crossentropy,\n",
    "#                  metrics=[metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*****Fitting LSTM Model 1******\")\n",
    "\n",
    "history_case_1 = lstm_model_1.fit(x=train_input_sub_1, y=y_train_sub_1, batch_size=batch_size,\n",
    "                        epochs=500, validation_data=(val_input_1, y_val_1), verbose=False, callbacks=[early_stopping,reduce_lr])\n",
    "# print(\"*****Fitting LSTM Model 2******\")\n",
    "# history_case_2 = lstm_model_2.fit(x=train_input_sub_2, y=y_train_sub_2, batch_size=batch_size,\n",
    "#                     epochs=50, validation_data=(val_input_2, y_val_2), verbose=False, callbacks=[early_stopping,reduce_lr])\n",
    "# print(\"*****Fitting LSTM Model 3******\")\n",
    "# history_case_3 = lstm_model_3.fit(x=train_input_sub_3, y=y_train_sub_3, batch_size=batch_size,\n",
    "#                     epochs=50, validation_data=(val_input_3, y_val_3), verbose=False, callbacks=[early_stopping,reduce_lr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot history\n",
    "def plot_history(history, subtitle,output_graph_path):\n",
    "    epoches = np.arange(1, len(history.history.get('loss')) + 1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15, 7))  # Removed sharey=True\n",
    "\n",
    "    # Extract Loss Values\n",
    "    train_loss = history.history.get('loss')\n",
    "    val_loss = history.history.get('val_loss')\n",
    "\n",
    "    # Extract Accuracy Values (Scaled to Percentage)\n",
    "    train_acc = [x * 100 for x in history.history.get('categorical_accuracy')]\n",
    "    val_acc = [x * 100 for x in history.history.get('val_categorical_accuracy')]\n",
    "\n",
    "    # --- Plot Training & Validation Loss ---\n",
    "    ax1.plot(epoches, train_loss, 'b', marker='o', label='Training Loss')\n",
    "    ax1.plot(epoches, val_loss, 'r', marker='o', label='Validation Loss')\n",
    "    ax1.set_title('Training and Validation Loss: ' + subtitle, fontsize=16)\n",
    "    ax1.set_ylabel('Loss', fontsize=14)\n",
    "    ax1.legend(fontsize=13)\n",
    "    ax1.tick_params(axis='x', labelsize=14)  \n",
    "    ax1.tick_params(axis='y', labelsize=14)\n",
    "    \n",
    "    # Dynamically adjust the Y-Axis range for loss\n",
    "    ax1.set_ylim(min(train_loss + val_loss) * 0.9, max(train_loss + val_loss) * 1.1)\n",
    "    ax1.set_yticks(np.linspace(min(train_loss + val_loss), max(train_loss + val_loss), num=5))  # 5 evenly spaced ticks\n",
    "\n",
    "    # --- Plot Training & Validation Accuracy ---\n",
    "    ax2.plot(epoches, train_acc, 'b', marker='o', label='Training Accuracy')\n",
    "    ax2.plot(epoches, val_acc, 'r', marker='o', label='Validation Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy: ' + subtitle, fontsize=16)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=14)  # Accuracy in Percentage\n",
    "    ax2.legend(fontsize=13)\n",
    "    ax2.tick_params(axis='x', labelsize=14)  \n",
    "    ax2.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "    # Dynamically adjust the Y-Axis range for accuracy\n",
    "    ax2.set_ylim(min(train_acc + val_acc) * 0.9, max(train_acc + val_acc) * 1.1)\n",
    "    ax2.set_yticks(np.linspace(min(train_acc + val_acc), max(train_acc + val_acc), num=5))  # 5 evenly spaced ticks\n",
    "    plt.savefig(output_graph_path, format='png')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_case_1,\"Case 1\",output_graph_path)\n",
    "\n",
    "# plot_history(history_case_2, \"Case 2\")\n",
    "# plot_history(history_case_3, \"Case 3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance evaluation\n",
    "Overall loss and accuracy of the model was calculated using the 'evaluate' method. The loss and accuracy for each class was also calculated using the 'calc_metrics' function of the 'img_util' module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(lstm_model, test_input, y_test, subtitle):\n",
    "    overall_loss, overal_accu = lstm_model.evaluate(test_input, y_test, verbose=False)\n",
    "    print(f'{subtitle}')\n",
    "    print('Overall loss = {}'.format(overall_loss))\n",
    "    print('Overall accuracy = {}\\n'.format(overal_accu))\n",
    "\n",
    "    # uncomment following for calculating and printing accuracy per class\n",
    "    # model_metrics = util.calc_metrics(lstm_model, test_input,\n",
    "    #                                         y_test, int_to_vector_dict, verbose=False)\n",
    "    \n",
    "    # print('{}{:>13}\\n{}'.format('Class ID','Accuracy', 30*'_'))\n",
    "    # for key, val in model_metrics.items():\n",
    "    #     print(('{:>2d}{:>18.4f}\\n'+'{}').format(key, val[0][1], 30*'_'))\n",
    "\n",
    "\n",
    "evaluate_model(lstm_model_1,test_input_1, y_test_1, \"Case 1\")\n",
    "# evaluate_model(lstm_model_2,test_input_2, y_test_2, \"Case 2\")\n",
    "# evaluate_model(lstm_model_3,test_input_3, y_test_3,\"Case 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "lstm_model_1.save(output_model_path)  # Saves in HDF5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions using test data\n",
    "The trained model was used for label predictions using the training, validation, and test datasets. It was also used to make label prediction for the entire dataset including unlabeled pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.colors as mcolors\n",
    "# def make_prediction(lstm_model,data_set,train_input_sub, val_input, test_input,y_train_sub, y_val, y_test):\n",
    "#     # Plotting predicted results\n",
    "#     concat_rows =  np.concatenate((train_rows_sub, val_rows, test_rows))\n",
    "#     concat_cols = np.concatenate((train_cols_sub, val_cols, test_cols))\n",
    "#     concat_input = np.concatenate((train_input_sub, val_input, test_input))\n",
    "#     concat_y = np.concatenate((y_train_sub, y_val, y_test))\n",
    "#     pixel_indices = (concat_rows, concat_cols)\n",
    "\n",
    "#     partial_map = util.plot_partial_map(lstm_model, gt, pixel_indices, concat_input,\n",
    "#                                 concat_y, int_to_vector_dict, plo=False)\n",
    "#     full_map,legend = util.plot_full_map(lstm_model, data_set, gt, int_to_vector_dict, patch_size, plo=False)\n",
    "\n",
    "#     return partial_map, full_map, legend\n",
    "\n",
    "# def plot_and_save_result(partial_map, full_map, unique_classes, subtitle):\n",
    "\n",
    "#     print(np.unique(partial_map))\n",
    "#     print(np.unique(full_map))\n",
    "#     print(unique_classes.keys())\n",
    "\n",
    "#     handles = []\n",
    "#     labels = []\n",
    "#     cmap = plt.get_cmap('tab20')  # You can change the colormap if you prefer tab10\n",
    "#     colors = [cmap(i / len(unique_classes)) for i in range(len(unique_classes))]\n",
    "#     for i, class_label in enumerate(unique_classes):\n",
    "#         handles.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], markersize=10))\n",
    "#         labels.append(classes[class_label])\n",
    "#     fixed_cmap = mcolors.ListedColormap(colors)  # Fixed colormap\n",
    "#     norm = mcolors.BoundaryNorm(list(unique_classes.keys()) + [max(unique_classes) + 1], fixed_cmap.N)  # Normalize class values\n",
    "\n",
    "    \n",
    "#     fig, (ax1, ax2) = plt.subplots(1,2,sharex=True, sharey=True, figsize=(15,7))\n",
    "#     ax1.imshow(partial_map,cmap=fixed_cmap,norm=norm,interpolation=\"nearest\")\n",
    "#     ax1.set_title(f'Prediction map for labeled data: {subtitle}', fontweight=\"bold\", fontsize='14')\n",
    "#     ax1.legend(handles=handles, labels=labels, loc='lower right', title=\"Classes\",fontsize=\"x-small\")\n",
    "\n",
    "#     ax2.imshow(full_map, cmap=fixed_cmap,norm=norm, interpolation=\"nearest\")\n",
    "#     ax2.set_title(f'Prediction map for all data: {subtitle}', fontweight=\"bold\", fontsize='14')\n",
    "#     # ax2.legend(handles=handles, labels=labels, loc='lower right', title=\"Classes\",fontsize=\"x-small\")\n",
    "#     ax2.set_title(f\"Prediction map for all data: {subtitle}\", fontweight=\"bold\", fontsize=14)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Save the output classified image\n",
    "#     full_map_tiff = full_map.astype(np.uint8)  #int is not supported\n",
    "#     # Example values: Replace with your actual georeference information\n",
    "#     transform = from_origin(0, 145, 1, 1)  # (top-left x, top-left y, pixel width, pixel height)\n",
    "\n",
    "#     # Save full_map as a .tif file\n",
    "#     with rasterio.open(\n",
    "#         f'output/classified_MAP_LSTM_{subtitle}.tif',\n",
    "#         'w',\n",
    "#         driver='GTiff',\n",
    "#         height=full_map_tiff.shape[0],\n",
    "#         width=full_map_tiff.shape[1],\n",
    "#         count=1,  # Number of bands\n",
    "#         dtype=full_map_tiff.dtype,\n",
    "#         crs='EPSG:32633',  # Coordinate reference system (adjust as needed)\n",
    "#         transform=transform,\n",
    "#     ) as dst:\n",
    "#         dst.write(full_map_tiff, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial_map_1, full_map_1, unique_classes_1 = make_prediction(lstm_model_1,data_set_1,train_input_sub_1, val_input_1, test_input_1, y_train_sub_1, y_val_1, y_test_1)\n",
    "# partial_map_2, full_map_2, unique_classes_2 = make_prediction(lstm_model_2,data_set_2,train_input_sub_2, val_input_2, test_input_2, y_train_sub_2, y_val_2, y_test_2)\n",
    "# partial_map_3, full_map_3, unique_classes_3 = make_prediction(lstm_model_3,data_set_3,train_input_sub_3, val_input_3, test_input_3, y_train_sub_3, y_val_3, y_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_and_save_result(partial_map_1, full_map_1, unique_classes_1,'Case 1 (PCA 15)')\n",
    "# plot_and_save_result(partial_map_2, full_map_2, unique_classes_2,'Case 2 (PCA 25)')\n",
    "# plot_and_save_result(partial_map_3, full_map_3, unique_classes_3,'Case 3 (PCA 69)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend is valid for corresponding prediction map as well.\n",
    "\n",
    "The salt-and-pepper noise is more in case 1 and case 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mul patch size 5\n",
    "# Overall accuracy = 0.74\n",
    "\n",
    "# patch size 5\n",
    "# Overall accuracy = 0.74\n",
    "\n",
    "# Case 1: mul patch size 7\n",
    "# Overall loss = 0.5315318703651428\n",
    "# Overall accuracy = 0.8064724802970886\n",
    "\n",
    "# Case 1: mul patch size 3\n",
    "# Overall loss = 0.5237630009651184\n",
    "# Overall accuracy = 0.768284797668457\n",
    "\n",
    "# Case 1: CNN LSTM\n",
    "# Overall loss = 0.5467633008956909\n",
    "# Overall accuracy = 0.7741100192070007"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fire_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
